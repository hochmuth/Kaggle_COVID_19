{"cells":[{"metadata":{},"cell_type":"markdown","source":"# COVID19 - April 2020 Forecast\n### A simple LSTM to predict time series"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom keras.models import Sequential\nfrom keras.layers import LSTM\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.layers import RepeatVector\nfrom keras.layers import TimeDistributed\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-2/train.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploration"},{"metadata":{"trusted":true},"cell_type":"code","source":"display(train_df.head())\ndisplay(train_df.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('We have', len(train_df.Country_Region.unique()), 'countries/regions in the dataset.')\nprint('We have', len(train_df.Province_State.unique()), 'provinces/states in the dataset.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_fatalities = train_df.loc[train_df['Date'] == train_df['Date'].max()]['Fatalities'].sum()\nprint('Between {} and {} there are {} fatalities.'.format(train_df['Date'].min(), train_df['Date'].max(), int(num_fatalities)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cz_sick = int(train_df.loc[(train_df['Country_Region'] == 'Czechia') & (train_df['Date'] == train_df['Date'].max())]['ConfirmedCases'].values[0])\ncz_fatalities = int(train_df.loc[(train_df['Country_Region'] == 'Czechia') & (train_df['Date'] == train_df['Date'].max())]['Fatalities'].values[0])\nprint('In Czech Republic there are {} confirmed cases and {} fatalities.'.format(cz_sick, cz_fatalities))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"timetrend_sick = sns.lineplot(train_df['Date'], train_df['ConfirmedCases'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"timetrend_deceased = sns.lineplot(train_df['Date'], train_df['Fatalities'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Transformation, Pre-processing"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add a new column to be able to distinguish regions\n\ntrain_df['UniqueRegion'] = np.where(train_df['Province_State'].isna(), train_df['Country_Region'], train_df['Country_Region'] + ' - ' + train_df['Province_State'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate number of new sick per day\n\ncountries = train_df['UniqueRegion'].unique()\ntrain_df['SickPerDay'] = 0\n\nbaseline_length = len(train_df.loc[train_df['UniqueRegion'] == 'Afghanistan']) # Country chosen arbitrarily\n\nfor country in countries:\n    len_country = len(train_df.loc[train_df['UniqueRegion'] == country])\n    len_diffs = len(train_df.loc[train_df['UniqueRegion'] == country]['ConfirmedCases'].diff())\n    if len_country > baseline_length or len_diffs > baseline_length:\n        raise NameError('Too many rows for country {}'.format(country))\n    train_df['SickPerDay'].loc[(train_df['UniqueRegion'] == country)] = train_df.loc[train_df['UniqueRegion'] == country]['ConfirmedCases'].diff()\n    \ntrain_df['SickPerDay'] = train_df['SickPerDay'].fillna(0)\n\n# Show an example\ndisplay(train_df.loc[train_df['UniqueRegion'] == 'Czechia'].tail())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lineplot(train_df.loc[(train_df['UniqueRegion'] == 'Czechia') & (train_df['ConfirmedCases'] > 0)]['Date'], train_df.loc[(train_df['UniqueRegion'] == 'Czechia')& (train_df['ConfirmedCases'] > 0)]['ConfirmedCases'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lineplot(x=train_df.loc[(train_df['UniqueRegion'] == 'Czechia') & (train_df['SickPerDay'] > 0)]['Date'],\n             y=train_df.loc[(train_df['UniqueRegion'] == 'Czechia')& (train_df['SickPerDay'] > 0)]['SickPerDay'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top10_most_cases = train_df.loc[train_df['Date'] == train_df['Date'].max()][['UniqueRegion','ConfirmedCases']].sort_values(by='ConfirmedCases', ascending=False).head(10)\ntop10_most_deceased = train_df.loc[train_df['Date'] == train_df['Date'].max()][['UniqueRegion','Fatalities']].sort_values(by='Fatalities', ascending=False).head(10)\ntop10_most_sick_per_day = train_df.loc[train_df['Date'] == train_df['Date'].max()][['UniqueRegion','SickPerDay']].sort_values(by='SickPerDay', ascending=False).head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top10_most_cases_df = train_df.loc[train_df['UniqueRegion'].isin(top10_most_cases['UniqueRegion'].values)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transform the dataframe to show each country in a different column\n\nmain_df = pd.DataFrame()\n\nfor i, top10_country in enumerate (top10_most_cases_df['UniqueRegion'].unique()):\n    if i == 0:\n        main_df = top10_most_cases_df.loc[top10_most_cases_df['UniqueRegion'] == top10_country][['Date', 'ConfirmedCases']].sort_values(by='Date')\n        main_df = main_df.rename({'ConfirmedCases': top10_country}, axis='columns')\n\n    else:\n        temp_df = top10_most_cases_df.loc[top10_most_cases_df['UniqueRegion'] == top10_country][['Date', 'ConfirmedCases']]\n        temp_df = temp_df.rename({'ConfirmedCases': top10_country}, axis='columns')\n        main_df = pd.merge(main_df, temp_df, on=['Date'])\n\nmain_df = main_df.set_index('Date')\nmain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"main_df.plot(figsize=(20,10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transform main data into a horizontal dataframe\n\ndef transform_horizontally(input_df, value_column):\n\n    horizontal_df = pd.DataFrame()\n\n    for i, uniqueRegion in enumerate (input_df['UniqueRegion'].unique()):\n        if i == 0:\n            horizontal_df = input_df.loc[input_df['UniqueRegion'] == uniqueRegion][['Date', value_column]].sort_values(by='Date')\n            horizontal_df = horizontal_df.rename({value_column: uniqueRegion}, axis='columns')\n\n        else:\n            temp_df = input_df.loc[train_df['UniqueRegion'] == uniqueRegion][['Date', value_column]]\n            temp_df = temp_df.rename({value_column: uniqueRegion}, axis='columns')\n            horizontal_df = pd.merge(horizontal_df, temp_df, on=['Date'])\n            \n    return horizontal_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confirmed_horizontal_df = transform_horizontally(train_df, 'ConfirmedCases').sort_values(by='Date')\nfatalities_horizontal_df = transform_horizontally(train_df, 'Fatalities').sort_values(by='Date')\n\n\ndisplay(confirmed_horizontal_df.head())\ndisplay(confirmed_horizontal_df.shape)\n\ndisplay(fatalities_horizontal_df.head())\ndisplay(fatalities_horizontal_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert dataframes into numpy arrays\n\nnp_confirmed = confirmed_horizontal_df.drop(columns=['Date']).to_numpy()\nnp_confirmed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scale the values (better performance of LSTM)\nscaler = MinMaxScaler(feature_range = (0, 1))\nnp_confirmed_scaled = scaler.fit_transform(np_confirmed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split a multivariate sequence into samples\n# Credits to: https://machinelearningmastery.com/how-to-develop-lstm-models-for-time-series-forecasting\n\ndef split_sequences(sequences, n_steps_in, n_steps_out):\n    X, y = list(), list()\n    for i in range(len(sequences)):\n        # find the end of this pattern\n        end_ix = i + n_steps_in\n        out_end_ix = end_ix + n_steps_out\n        # check if we are beyond the dataset\n        if out_end_ix > len(sequences):\n            break\n        # gather input and output parts of the pattern\n        seq_x, seq_y = sequences[i:end_ix, :], sequences[end_ix:out_end_ix, :]\n        X.append(seq_x)\n        y.append(seq_y)\n    return np.array(X), np.array(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_steps_in = 11\nn_steps_out = 1\n\nX, y = split_sequences(np_confirmed_scaled, n_steps_in, n_steps_out)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_features = X.shape[2]\ndisplay(n_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define model\nmodel = Sequential()\nmodel.add(LSTM(500, activation='relu', input_shape=(n_steps_in, n_features)))\nmodel.add(RepeatVector(n_steps_out))\nmodel.add(LSTM(1000, activation='relu', return_sequences=True))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(1000, activation='relu', return_sequences=True))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(1000, activation='relu', return_sequences=True))\nmodel.add(TimeDistributed(Dense(n_features)))\nmodel.compile(optimizer='adam', loss='mse')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X, y, epochs=300, verbose=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_pred = np_confirmed_scaled[-n_steps_in-1:-n_steps_out].reshape((1, n_steps_in, n_features))\ny_pred = model.predict(X_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(list(np_confirmed[-1]))\nrounded_pred = [int(x) for x in scaler.inverse_transform(y_pred[0])[0].astype(int)]\nprint(rounded_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}